<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Decision Tree Challenge</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="index_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="index_files/libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap-9e3ffae467580fdb927a41352e75a2e0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Decision Tree Challenge</h1>
<p class="subtitle lead">Feature Importance and Categorical Variable Encoding</p>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="decision-tree-challenge---feature-importance-and-variable-encoding" class="level1">
<h1>üå≥ Decision Tree Challenge - Feature Importance and Variable Encoding</h1>
<section id="challenge-overview" class="level2">
<h2 class="anchored" data-anchor-id="challenge-overview">Challenge Overview</h2>
<p><strong>Your Mission:</strong> Create a simple GitHub Pages site that demonstrates how decision trees measure feature importance and analyzes the critical differences between categorical and numerical variable encoding. You‚Äôll answer two key discussion questions by adding narrative to a pre-built analysis and posting those answers to your GitHub Pages site as a rendered HTML document.</p>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>‚ö†Ô∏è AI Partnership Required
</div>
</div>
<div class="callout-body-container callout-body">
<p>This challenge pushes boundaries intentionally. You‚Äôll tackle problems that normally require weeks of study, but with Cursor AI as your partner (and your brain keeping it honest), you can accomplish more than you thought possible.</p>
<p><strong>The new reality:</strong> The four stages of competence are Ignorance ‚Üí Awareness ‚Üí Learning ‚Üí Mastery. AI lets us produce Mastery-level work while operating primarily in the Awareness stage. I focus on awareness training, you leverage AI for execution, and together we create outputs that used to require years of dedicated study.</p>
</div>
</div>
</section>
<section id="the-decision-tree-problem" class="level2">
<h2 class="anchored" data-anchor-id="the-decision-tree-problem">The Decision Tree Problem üéØ</h2>
<blockquote class="blockquote">
<p>‚ÄúThe most important thing in communication is hearing what isn‚Äôt said.‚Äù - Peter Drucker</p>
</blockquote>
<p><strong>The Core Problem:</strong> Decision trees are often praised for their interpretability and ability to handle both numerical and categorical variables. But what happens when we encode categorical variables as numbers? How does this affect our understanding of feature importance?</p>
<p><strong>What is Feature Importance?</strong> In decision trees, feature importance measures how much each variable contributes to reducing impurity (or improving prediction accuracy) across all splits in the tree. It‚Äôs a key metric for understanding which variables matter most for your predictions.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>üéØ The Key Insight: Encoding Matters for Interpretability
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>The problem:</strong> When we encode categorical variables as numerical values (like 1, 2, 3, 4‚Ä¶), decision trees treat them as if they have a meaningful numerical order. This can completely distort our analysis.</p>
<p><strong>The Real-World Context:</strong> In real estate, we know that neighborhood quality, house style, and other categorical factors are crucial for predicting home prices. But if we encode these as numbers, we might get misleading insights about which features actually matter most.</p>
<p><strong>The Devastating Reality:</strong> Even sophisticated machine learning models can give us completely wrong insights about feature importance if we don‚Äôt properly encode our variables. A categorical variable that should be among the most important might appear irrelevant, while a numerical variable might appear artificially important.</p>
</div>
</div>
<p>Let‚Äôs assume we want to predict house prices and understand which features matter most. The key question is: <strong>How does encoding categorical variables as numbers affect our understanding of feature importance?</strong></p>
</section>
<section id="the-ames-housing-dataset" class="level2">
<h2 class="anchored" data-anchor-id="the-ames-housing-dataset">The Ames Housing Dataset üè†</h2>
<p>We are analyzing the Ames Housing dataset which contains detailed information about residential properties sold in Ames, Iowa from 2006 to 2010. This dataset is perfect for our analysis because it contains a categorical variable (like zip code) and numerical variables (like square footage, year built, number of bedrooms).</p>
</section>
<section id="the-problem-zipcode-as-numerical-vs-categorical" class="level2">
<h2 class="anchored" data-anchor-id="the-problem-zipcode-as-numerical-vs-categorical">The Problem: ZipCode as Numerical vs Categorical</h2>
<p><strong>Key Question:</strong> What happens when we treat zipCode as a numerical variable in a decision tree? How does this affect feature importance interpretation?</p>
<p><strong>The Issue:</strong> Zip codes (50010, 50011, 50012, 50013) are categorical variables representing discrete geographic areas, i.e.&nbsp;neighborhoods. When treated as numerical, the tree might split on ‚ÄúzipCode &gt; 50012.5‚Äù - which has no meaningful interpretation for house prices. Zip codes are non-ordinal categorical variables meaning they have no inherent order that aids house price prediction (i.e.&nbsp;zip code 99999 is not the priceiest zip code).</p>
</section>
<section id="data-loading-and-model-building" class="level2">
<h2 class="anchored" data-anchor-id="data-loading-and-model-building">Data Loading and Model Building</h2>
<section id="python-code" class="level3">
<h3 class="anchored" data-anchor-id="python-code">Python Code</h3>
<div id="26ee361f" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor, plot_tree</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">'ignore'</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>sales_data <span class="op">=</span> pd.read_csv(<span class="st">"salesPriceData.csv"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare model data (treating zipCode as numerical)</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>model_vars <span class="op">=</span> [<span class="st">'SalePrice'</span>, <span class="st">'LotArea'</span>, <span class="st">'YearBuilt'</span>, <span class="st">'GrLivArea'</span>, <span class="st">'FullBath'</span>, </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>              <span class="st">'HalfBath'</span>, <span class="st">'BedroomAbvGr'</span>, <span class="st">'TotRmsAbvGrd'</span>, <span class="st">'GarageCars'</span>, <span class="st">'zipCode'</span>]</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>model_data <span class="op">=</span> sales_data[model_vars].dropna()</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Split data</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> model_data.drop(<span class="st">'SalePrice'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> model_data[<span class="st">'SalePrice'</span>]</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Build decision tree</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>tree_model <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>, </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                                  min_samples_split<span class="op">=</span><span class="dv">20</span>, </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>                                  min_samples_leaf<span class="op">=</span><span class="dv">10</span>, </span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>                                  random_state<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>tree_model.fit(X_train, y_train)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Model built with </span><span class="sc">{</span>tree_model<span class="sc">.</span>get_n_leaves()<span class="sc">}</span><span class="ss"> terminal nodes"</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model built with 8 terminal nodes</code></pre>
</div>
</div>
</section>
</section>
<section id="tree-visualization" class="level2">
<h2 class="anchored" data-anchor-id="tree-visualization">Tree Visualization</h2>
<section id="python" class="level3">
<h3 class="anchored" data-anchor-id="python">Python</h3>
<div id="cell-visualize-tree-python" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize tree</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>plot_tree(tree_model, </span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>          feature_names<span class="op">=</span>X_train.columns,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>          filled<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>          rounded<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>          fontsize<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>          max_depth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree (zipCode as Numerical)"</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/visualize-tree-python-output-1.png" id="visualize-tree-python" width="942" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>:::</p>
</section>
</section>
<section id="feature-importance-analysis" class="level2">
<h2 class="anchored" data-anchor-id="feature-importance-analysis">Feature Importance Analysis</h2>
<section id="python-1" class="level3">
<h3 class="anchored" data-anchor-id="python-1">Python</h3>
<div id="cell-importance-plot-python" class="cell" data-fig-height="5" data-fig-width="8" data-execution_count="4">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/importance-plot-python-output-1.png" id="importance-plot-python" width="758" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="critical-analysis-the-encoding-problem" class="level2">
<h2 class="anchored" data-anchor-id="critical-analysis-the-encoding-problem">Critical Analysis: The Encoding Problem</h2>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>‚ö†Ô∏è The Problem Revealed
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>What to note:</strong> Our decision tree treated <code>zipCode</code> as a numerical variable. This leads to zip code being unimportant. Not surprisingly, because there is no reason to believe allowing splits like ‚ÄúzipCode &lt; 50012.5‚Äù should be beneficial for house price prediction. This false coding of a variable creates several problems:</p>
<ol type="1">
<li><strong>Potentially Meaningless Splits:</strong> A zip code of 50013 is not ‚Äúgreater than‚Äù 50012 in any meaningful way for house prices</li>
<li><strong>False Importance:</strong> The algorithm assigns importance to zipCode based on numerical splits rather than categorical distinctions OR the importance of zip code is completely missed as numerical ordering has no inherent relationship to house prices.</li>
<li><strong>Misleading Interpretations:</strong> We might conclude zipCode is not important when our intuition tells us it should be important (listen to your intuition).</li>
</ol>
<p><strong>The Real Issue:</strong> Zip codes are categorical variables representing discrete geographic areas. The numerical values have no inherent order or magnitude relationship to house prices. These must be modelled as categorical variables.</p>
</div>
</div>
</section>
<section id="proper-categorical-encoding-the-solution" class="level2">
<h2 class="anchored" data-anchor-id="proper-categorical-encoding-the-solution">Proper Categorical Encoding: The Solution</h2>
<p>Now let‚Äôs repeat the analysis with zipCode properly encoded as categorical variables to see the difference.</p>
<p><strong>Python Approach:</strong> One-hot encode zipCode (create dummy variables for each zip code)</p>
<section id="categorical-encoding-analysis" class="level3">
<h3 class="anchored" data-anchor-id="categorical-encoding-analysis">Categorical Encoding Analysis</h3>
</section>
<section id="python-2" class="level3">
<h3 class="anchored" data-anchor-id="python-2">Python</h3>
</section>
<section id="tree-visualization-categorical-zipcode" class="level3">
<h3 class="anchored" data-anchor-id="tree-visualization-categorical-zipcode">Tree Visualization: Categorical zipCode</h3>
</section>
<section id="python-3" class="level3">
<h3 class="anchored" data-anchor-id="python-3">Python</h3>
<div id="cell-visualize-tree-cat-python" class="cell" data-fig-height="6" data-fig-width="10" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize tree with one-hot encoded zipCode</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plot_tree(tree_model_cat, </span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>          feature_names<span class="op">=</span>X_train_cat.columns,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>          filled<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>          rounded<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>          fontsize<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>          max_depth<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree (zipCode One-Hot Encoded)"</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/visualize-tree-cat-python-output-1.png" id="visualize-tree-cat-python" width="938" height="566" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="feature-importance-categorical-zipcode" class="level3">
<h3 class="anchored" data-anchor-id="feature-importance-categorical-zipcode">Feature Importance: Categorical zipCode</h3>
</section>
<section id="python-4" class="level3">
<h3 class="anchored" data-anchor-id="python-4">Python</h3>
<div id="cell-importance-plot-cat-python" class="cell" data-fig-height="5" data-fig-width="8" data-execution_count="7">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/importance-plot-cat-python-output-1.png" id="importance-plot-cat-python" width="757" height="470" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="challenge-requirements" class="level2">
<h2 class="anchored" data-anchor-id="challenge-requirements">Challenge Requirements üìã</h2>
<section id="minimum-requirements-for-any-points-on-challenge" class="level3">
<h3 class="anchored" data-anchor-id="minimum-requirements-for-any-points-on-challenge">Minimum Requirements for Any Points on Challenge</h3>
<ol type="1">
<li><p><strong>Create a GitHub Pages Site:</strong> Use the starter repository (see Repository Setup section below) to begin with a working template. The repository includes all the analysis code and visualizations above.</p></li>
<li><p><strong>Add Discussion Narrative:</strong> Add your answers to the two discussion questions below in the Discussion Questions section of the rendered HTML.</p></li>
<li><p><strong>GitHub Repository:</strong> Use your forked repository (from the starter repository) named ‚ÄúdecTreeChallenge‚Äù in your GitHub account.</p></li>
<li><p><strong>GitHub Pages Setup:</strong> The repository should be made the source of your github pages:</p>
<ul>
<li>Go to your repository settings (click the ‚ÄúSettings‚Äù tab in your GitHub repository)</li>
<li>Scroll down to the ‚ÄúPages‚Äù section in the left sidebar</li>
<li>Under ‚ÄúSource‚Äù, select ‚ÄúDeploy from a branch‚Äù</li>
<li>Choose ‚Äúmain‚Äù branch and ‚Äú/ (root)‚Äù folder</li>
<li>Click ‚ÄúSave‚Äù</li>
<li>Your site will be available at: <code>https://[your-username].github.io/decTreeChallenge/</code></li>
<li><strong>Note:</strong> It may take a few minutes for the site to become available after enabling Pages</li>
</ul></li>
</ol>
</section>
</section>
<section id="getting-started-repository-setup" class="level2">
<h2 class="anchored" data-anchor-id="getting-started-repository-setup">Getting Started: Repository Setup üöÄ</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>üìÅ Quick Start with Starter Repository
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Step 1:</strong> Fork the starter repository to your github account at <a href="https://github.com/flyaflya/decTreeChallenge.git">https://github.com/flyaflya/decTreeChallenge.git</a></p>
<p><strong>Step 2:</strong> Clone your fork locally using Cursor (or VS Code)</p>
<p><strong>Step 3:</strong> You‚Äôre ready to start! The repository includes pre-loaded data and a working template with all the analysis above.</p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Tip</span>üí° Why Use the Starter Repository?
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Pre-loaded data:</strong> All required data and analysis code is included</li>
<li><strong>Working template:</strong> Basic Quarto structure (<code>index.qmd</code>) is ready</li>
<li><strong>No setup errors:</strong> Avoid common data loading issues</li>
<li><strong>Focus on analysis:</strong> Spend time on the discussion questions, not data preparation</li>
</ul>
</div>
</div>
<section id="getting-started-tips" class="level3">
<h3 class="anchored" data-anchor-id="getting-started-tips">Getting Started Tips</h3>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Note</span>üéØ Navy SEALs Motto
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>‚ÄúSlow is Smooth and Smooth is Fast‚Äù</p>
</blockquote>
<p><em>Take your time to understand the decision tree mechanics, plan your approach carefully, and execute with precision. Rushing through this challenge will only lead to errors and confusion.</em></p>
</div>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Warning</span>üíæ Important: Save Your Work Frequently!
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Before you start:</strong> Make sure to commit your work often using the Source Control panel in Cursor (Ctrl+Shift+G or Cmd+Shift+G). This prevents the AI from overwriting your progress and ensures you don‚Äôt lose your work.</p>
<p><strong>Commit after each major step:</strong></p>
<ul>
<li>After adding your discussion answers</li>
<li>After rendering to HTML</li>
<li>Before asking the AI for help with new code</li>
</ul>
<p><strong>How to commit:</strong></p>
<ol type="1">
<li>Open Source Control panel (Ctrl+Shift+G)</li>
<li>Stage your changes (+ button)</li>
<li>Write a descriptive commit message</li>
<li>Click the checkmark to commit</li>
</ol>
<p><em>Remember: Frequent commits are your safety net!</em></p>
</div>
</div>
</section>
</section>
<section id="discussion-questions-for-challenge" class="level2">
<h2 class="anchored" data-anchor-id="discussion-questions-for-challenge">Discussion Questions for Challenge</h2>
<p><strong>Your Task:</strong> Add thoughtful narrative answers to these two questions in the Discussion Questions section of your rendered HTML site.</p>
<ol type="1">
<li><strong>Numerical vs Categorical Encoding:</strong> There are two modelsin Python written above. For each language, the models differ by how zip code is modelled, either as a numerical variable or as a categorical variable. Given what you know about zip codes and real estate prices, how should zip code be modelled, numerically or categorically? Is zipcode and ordinal or non-ordinal variable?</li>
</ol>
<p>Answer :</p>
<p>zipCode should be modelled as a categorical (non-ordinal) variable, not a numerical one. Zip codes are labels for geographic regions and have no natural numeric ordering that implies higher/lower value for the target. Treating them as numeric lets a tree create splits like zipCode &lt;= 50012.5, which is meaningless and can produce misleading feature importance.</p>
<p>Why:</p>
<p>Decision trees that see numeric-encoded categories (e.g., 50010, 50011, 50012) will treat differences and ordering as meaningful and consider splits on numeric thresholds. That may cause either meaningless splits or under/over-estimation of the zipCode‚Äôs importance.</p>
<p>Proper categorical treatment (one-hot, or native categorical handling by specialized algorithms) lets the model evaluate splits that separate whole categories or groupings of categories, which is what we want for zip codes.</p>
<p>Short practical rule: zip codes = categorical, non-ordinal.</p>
<ol start="2" type="1">
<li><strong>R vs Python Implementation Differences:</strong> When modelling zip code as a categorical variable, the output tree and feature importance would differ quite significantly had you used R as opposed to Python. Investigate why this is the case. What does R offer that Python does not? Which language would you say does a better job of modelling zip code as a categorical variable? Can you quote the documentation at <a href="https://scikit-learn.org/stable/modules/tree.html">https://scikit-learn.org/stable/modules/tree.html</a> suggesting a weakness in the Python implementation? If so, please provide a quote from the documentation.</li>
</ol>
<p>Answer :</p>
<p>Historically, R‚Äôs tree implementations (e.g., rpart, tree) treat factor (categorical) variables natively and search splits across subsets of levels, while scikit-learn‚Äôs DecisionTree* historically did not natively support categorical features and required manual encoding (label or one-hot). This difference can lead to substantially different trees and feature importances between R and Python unless Python uses a library that supports categorical features natively.</p>
<p>Documentation quote from scikit-learn:</p>
<p>From the scikit-learn decision tree documentation:</p>
<p>‚ÄúAble to handle both numerical and categorical data. However, the scikit-learn implementation does not support categorical variables for now.‚Äù</p>
<p>This sentence in the docs is the key; it reflects that sklearn historically expects numeric input and the user must encode categoricals before fitting.</p>
<p>What R offers that Python (sklearn) historically does not:</p>
<p>R rpart and similar packages operate directly on R factors and will evaluate splits on discrete categories (they consider groupings of factor levels when searching splits). This gives theoretically more faithful handling of unordered categorical predictors in single-tree algorithms. See rpart vignette for details.</p>
<p>Which is better?</p>
<p>For a plain decision tree where we want native categorical splits, R (rpart) historically provided more direct support.</p>
<p>In Python, scikit-learn users commonly one-hot encode or use other encodings, which is valid but can change model shape and importance interpretation, and can be expensive with high-cardinality categories. See StackOverflow and the scikit-learn issue discussion for community context.</p>
<p>Practical takeaway: If we are using scikit-learn and want faithful categorical splitting, we either:</p>
<ul>
<li><p>One-hot encode and accept the tradeoffs, or</p></li>
<li><p>Use alternative Python libraries (LightGBM, CatBoost, etc.) that implement native categorical handling.</p></li>
</ul>
<ol start="3" type="1">
<li><strong>Are There Any Suggestions for Implementing Decision Trees in Python With Prioper Categorical Handling?</strong> Please poke around the Internet (AI is not as helpful with new libraries) for suggestions on how to implement decision trees in Python with better (i.e.&nbsp;not one-hot encoding) categorical handling. Please provide a link to the source and a quote from the source. There is not right answer here, but please provide a thoughtful answer, I am curious to see what you find.</li>
</ol>
<p>Answer:</p>
<p>A ‚Äî Use tree-based libraries with native categorical support (recommended):</p>
<p>CatBoost (Yandex) ‚Äî explicitly built to handle categorical features without one-hot encoding. Quote from CatBoost docs:</p>
<p>‚ÄúDo not use one-hot encoding during preprocessing. ‚Ä¶ CatBoost supports numerical, categorical, text, and embeddings features.‚Äù</p>
<p>LightGBM ‚Äî supports categorical_feature and uses specialized split-search for categories (often much faster and better than naive one-hot for high cardinality). Quote from LightGBM docs:</p>
<p>‚ÄúLightGBM offers good accuracy with integer-encoded categorical features. LightGBM applies [a categorical split algorithm] ‚Ä¶ Use categorical_feature to specify the categorical features.‚Äù</p>
<p>The best approach is to use tree-based libraries like CatBoost or LightGBM, which can handle categorical features directly without one-hot or label encoding. These libraries use special algorithms to split categories, so there‚Äôs no artificial ordering or feature explosion. This is especially useful for high-cardinality features like zip codes. In your results, using LightGBM produced meaningful splits, and the model gave reasonable importance to the zip code feature. This shows that categorical variables stayed categorical, which is exactly what these libraries are designed for. In short: use CatBoost or LightGBM whenever possible for best accuracy and simpler preprocessing.</p>
<p>Example :</p>
<div id="a3a8d8dd" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Numeric (bad) ‚Äî shows how trees act if you treat zip as numeric</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor, plot_tree</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"salesPriceData.csv"</span>)   <span class="co"># replace path</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>vars_ <span class="op">=</span> [<span class="st">'SalePrice'</span>,<span class="st">'LotArea'</span>,<span class="st">'YearBuilt'</span>,<span class="st">'GrLivArea'</span>,<span class="st">'FullBath'</span>,<span class="st">'HalfBath'</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>         <span class="st">'BedroomAbvGr'</span>,<span class="st">'TotRmsAbvGrd'</span>,<span class="st">'GarageCars'</span>,<span class="st">'zipCode'</span>]</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[vars_].dropna()</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">'SalePrice'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'SalePrice'</span>]</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">6</span>))</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>plot_tree(model, feature_names<span class="op">=</span>X_train.columns, filled<span class="op">=</span><span class="va">True</span>, rounded<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Decision Tree ‚Äî zipCode as NUMERIC (not recommended)"</span>)</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a><span class="co"># feature importances</span></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>fi <span class="op">=</span> pd.DataFrame({<span class="st">'feature'</span>: X_train.columns, <span class="st">'importance'</span>: model.feature_importances_})</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fi.sort_values(<span class="st">'importance'</span>, ascending<span class="op">=</span><span class="va">False</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="763" height="483" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>        feature  importance
2     GrLivArea    0.601614
7    GarageCars    0.259048
1     YearBuilt    0.139338
0       LotArea    0.000000
3      FullBath    0.000000
4      HalfBath    0.000000
5  BedroomAbvGr    0.000000
6  TotRmsAbvGrd    0.000000
8       zipCode    0.000000</code></pre>
</div>
</div>
<p>B ‚Äî Use target / frequency / CatBoost-style encoders (if we are using scikit-learn):</p>
<p>category_encoders library provides various encoders (TargetEncoder, CatBoostEncoder, CountEncoder). These reduce dimensionality compared to one-hot and often perform better than naive label encoding. (Search category_encoders docs / GitHub for implementation details.)</p>
<p>If we stay inside scikit-learn, categories can be encoded using OneHotEncoder or libraries like category_encoders (TargetEncoder, CatBoostEncoder, etc.). One-hot encoding avoids ordering, while target-based encoders reduce dimensionality. The pros are that these methods work everywhere without installing extra libraries. However, one-hot encoding can create many extra columns (for example, zipCode_50xxx), and decision trees often ignore most of them if the category effect is subtle. The results confirmed this: many one-hot features had zero importance, so this approach is okay but not ideal for high-cardinality features.</p>
<p>Example:</p>
<div id="c93d37e7" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"salesPriceData.csv"</span>).dropna(subset<span class="op">=</span>[<span class="st">'SalePrice'</span>, <span class="st">'zipCode'</span>])</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop(<span class="st">'SalePrice'</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'SalePrice'</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Categorical + numerical columns</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>cat_cols <span class="op">=</span> [<span class="st">'zipCode'</span>]</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>num_cols <span class="op">=</span> [c <span class="cf">for</span> c <span class="kw">in</span> X.columns <span class="cf">if</span> c <span class="kw">not</span> <span class="kw">in</span> cat_cols]</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Older sklearn DOES NOT support: sparse=False or drop=None</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># So we use the simplest safe version:</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>encoder <span class="op">=</span> OneHotEncoder(handle_unknown<span class="op">=</span><span class="st">'ignore'</span>)</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>ct <span class="op">=</span> ColumnTransformer(</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    transformers<span class="op">=</span>[</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        (<span class="st">"ohe_zip"</span>, encoder, cat_cols)</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    ],</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>    remainder<span class="op">=</span><span class="st">'passthrough'</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> Pipeline(steps<span class="op">=</span>[</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'encode'</span>, ct),</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'tree'</span>, DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">4</span>, random_state<span class="op">=</span><span class="dv">123</span>))</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>pipe.fit(X, y)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------</span></span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Get one-hot encoded feature names (works in all versions)</span></span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a><span class="co"># -------------------------------------------------------</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Get encoder from pipeline</span></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>ohe <span class="op">=</span> pipe.named_steps[<span class="st">'encode'</span>].named_transformers_[<span class="st">'ohe_zip'</span>]</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature names for OHE part</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    ohe_names <span class="op">=</span> <span class="bu">list</span>(ohe.get_feature_names_out(cat_cols))  <span class="co"># new sklearn</span></span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># fallback for older sklearn</span></span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>    ohe_names <span class="op">=</span> <span class="bu">list</span>(ohe.get_feature_names(cat_cols))</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Combine OHE names + numerical names</span></span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>all_features <span class="op">=</span> ohe_names <span class="op">+</span> num_cols</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>tree <span class="op">=</span> pipe.named_steps[<span class="st">'tree'</span>]</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature importances</span></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a>fi <span class="op">=</span> pd.DataFrame({</span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: all_features,</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance'</span>: tree.feature_importances_</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>})</span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fi.sort_values(<span class="st">'importance'</span>, ascending<span class="op">=</span><span class="va">False</span>).head(<span class="dv">20</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>          feature  importance
27      GrLivArea    0.623786
32     GarageCars    0.222443
26      YearBuilt    0.127823
6   zipCode_50016    0.018723
15  zipCode_50025    0.007225
0   zipCode_50010    0.000000
4   zipCode_50014    0.000000
3   zipCode_50013    0.000000
1   zipCode_50011    0.000000
8   zipCode_50018    0.000000
9   zipCode_50019    0.000000
11  zipCode_50021    0.000000
10  zipCode_50020    0.000000
12  zipCode_50022    0.000000
5   zipCode_50015    0.000000
7   zipCode_50017    0.000000
2   zipCode_50012    0.000000
16  zipCode_50026    0.000000
14  zipCode_50024    0.000000
13  zipCode_50023    0.000000</code></pre>
</div>
</div>
<p>C ‚Äî If staying with sklearn DecisionTree, prefer one-hot over label encoding for unordered categories:</p>
<p>One hot encoding avoids implying ordering but can explode feature count for many levels. Label encoding is dangerous for unordered categories, because it induces spurious ordering. (Community discussion: StackOverflow.)</p>
<p>Target encoding replaces each category with the mean of the target variable, like the average house price for each zip code. This creates only one new column per feature and captures strong signals. In the results, using target encoding made the zip code feature the most important, improved RMSE significantly, and produced more meaningful splits in the decision tree. This is the strongest and cleanest method if we are restricted to scikit-learn and can‚Äôt use CatBoost or LightGBM. In short: target encoding is simple, compact, and effective.</p>
<div id="6f58099d" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># C ‚Äî Target Encoding + Decision Tree (works everywhere, no extra installs)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">"salesPriceData.csv"</span>).dropna(subset<span class="op">=</span>[<span class="st">'SalePrice'</span>, <span class="st">'zipCode'</span>])</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Target Encode zipCode</span></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>zip_mean <span class="op">=</span> df.groupby(<span class="st">'zipCode'</span>)[<span class="st">'SalePrice'</span>].mean()</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'zipCode_TE'</span>] <span class="op">=</span> df[<span class="st">'zipCode'</span>].<span class="bu">map</span>(zip_mean)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># Features (drop original zipCode)</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df.drop([<span class="st">'SalePrice'</span>, <span class="st">'zipCode'</span>], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'SalePrice'</span>]</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Train/test split</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>    X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">123</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Decision Tree Model</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">123</span>)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Predictions</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co"># RMSE (manual, works in all sklearn versions)</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> mean_squared_error(y_test, y_pred) <span class="op">**</span> <span class="fl">0.5</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"RMSE: </span><span class="sc">{</span>rmse<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Feature importance</span></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>fi <span class="op">=</span> pd.DataFrame({</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">'feature'</span>: X_train.columns,</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">'importance'</span>: model.feature_importances_</span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>}).sort_values(<span class="st">'importance'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Top Feature Importances:"</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(fi.head(<span class="dv">20</span>))</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>RMSE: 38548.52

Top Feature Importances:
        feature  importance
8    zipCode_TE    0.442042
2     GrLivArea    0.307512
7    GarageCars    0.200034
5  BedroomAbvGr    0.015252
6  TotRmsAbvGrd    0.013391
0       LotArea    0.010885
1     YearBuilt    0.006802
3      FullBath    0.004083
4      HalfBath    0.000000</code></pre>
</div>
</div>
</section>
<section id="grading-rubric" class="level2">
<h2 class="anchored" data-anchor-id="grading-rubric">Grading Rubric üéì</h2>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
<span class="screen-reader-only">Important</span>üìä What You‚Äôre Really Being Graded On
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>This is an investigative report, not a coding exercise.</strong> You‚Äôre analyzing decision tree models and reporting your findings like a professional analyst would. Think of this as a brief you‚Äôd write for a client or manager about why proper variable encoding matters in machine learning.</p>
<p><strong>What makes a great report:</strong></p>
<ul>
<li><strong>Clear narrative:</strong> Tell the story of what you discovered about decision tree feature importance</li>
<li><strong>Insightful analysis:</strong> Focus on the most interesting differences between numerical and categorical encoding</li>
<li><strong>Professional presentation:</strong> Clean, readable, and engaging</li>
<li><strong>Concise conclusions:</strong> No AI babble or unnecessary technical jargon</li>
<li><strong>Human insights:</strong> Your interpretation of what the feature importance rankings actually mean (or don‚Äôt mean)</li>
<li><strong>Documentation-based analysis:</strong> For question 2, ground your analysis in actual library documentation</li>
</ul>
<p><strong>What we‚Äôre looking for:</strong> A compelling 1-2 minute read that demonstrates both the power of decision trees for interpretability and the critical importance of proper variable encoding. And a note on the current state of the art in decision tree implementation for categorical variables in Python.</p>
</div>
</div>
<section id="questions-to-answer-for-75-grade-on-challenge" class="level3">
<h3 class="anchored" data-anchor-id="questions-to-answer-for-75-grade-on-challenge">Questions to Answer for 75% Grade on Challenge</h3>
<ol type="1">
<li><strong>Numerical vs Categorical Analysis:</strong> Provide a clear, well-reasoned answer to question 1 about how zip codes should be modelled. Your answer should demonstrate understanding of why categorical variables need special treatment in decision trees.</li>
</ol>
</section>
<section id="questions-to-answer-for-85-grade-on-challenge" class="level3">
<h3 class="anchored" data-anchor-id="questions-to-answer-for-85-grade-on-challenge">Questions to Answer for 85% Grade on Challenge</h3>
<ol start="2" type="1">
<li><strong>R vs Python Implementation Analysis:</strong> Provide a thorough analysis of question 2, including investigation of the official documentation for both <code>rpart</code> (R) and <code>sklearn.tree.DecisionTreeRegressor</code> (Python). Your analysis should explain the technical differences and provide a reasoned opinion about which implementation handles categorical variables better. You do NOT have to run R-code.</li>
</ol>
</section>
<section id="questions-to-answer-for-95---100-grade-on-challenge" class="level3">
<h3 class="anchored" data-anchor-id="questions-to-answer-for-95---100-grade-on-challenge">Questions to Answer for 95% - 100% Grade on Challenge</h3>
<ol start="3" type="1">
<li><strong>Professional Presentation:</strong> Your discussion answers should be written in a professional, engaging style that would be appropriate for a business audience learning about one-hot encoding and decision trees. Avoid technical jargon and focus on practical implications. Include a specific quote from the official documentation of <code>sklearn.tree.DecisionTreeRegressor</code> that supports your analysis.</li>
</ol>
</section>
</section>
<section id="submission-checklist" class="level2">
<h2 class="anchored" data-anchor-id="submission-checklist">Submission Checklist ‚úÖ</h2>
<p><strong>Minimum Requirements (Required for Any Points):</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Forked starter repository from <a href="https://github.com/flyaflya/decTreeChallenge.git">https://github.com/flyaflya/decTreeChallenge.git</a></label></li>
<li><label><input type="checkbox">Cloned repository locally using Cursor (or VS Code)</label></li>
<li><label><input type="checkbox">Added thoughtful narrative answers to both discussion questions</label></li>
<li><label><input type="checkbox">Document rendered to HTML successfully</label></li>
<li><label><input type="checkbox">HTML files uploaded to your forked repository</label></li>
<li><label><input type="checkbox">GitHub Pages enabled and working</label></li>
<li><label><input type="checkbox">Site accessible at <code>https://[your-username].github.io/decTreeChallenge/</code></label></li>
</ul>
<p><strong>75% Grade Requirements:</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Clear, well-reasoned answer to question 1 about numerical vs categorical encoding</label></li>
</ul>
<p><strong>85% Grade Requirements:</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Thorough analysis of question 2 with investigation of official documentation</label></li>
</ul>
<p><strong>95% Grade Requirements:</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Professional presentation style appropriate for business audience.</label></li>
<li><label><input type="checkbox">Specific quote from official documentation of <code>sklearn.tree.DecisionTreeRegressor</code> supporting your analysis</label></li>
</ul>
<p><strong>100% Grade Requirements:</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Note on the current state of the art in decision tree implementation for categorical variables in Python.</label></li>
</ul>
<p><strong>Report Quality (Critical for Higher Grades):</strong></p>
<ul class="task-list">
<li><label><input type="checkbox">Clear, engaging narrative that tells a story</label></li>
<li><label><input type="checkbox">Focus on the most interesting findings about decision tree feature importance</label></li>
<li><label><input type="checkbox">Professional writing style (no AI-generated fluff)</label></li>
<li><label><input type="checkbox">Concise analysis that gets to the point</label></li>
<li><label><input type="checkbox">Practical insights that would help a real data scientist</label></li>
<li><label><input type="checkbox">Documentation-based analysis for technical questions</label></li>
</ul>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>